{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"../data/baidu_sentence.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>content</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>高中 生物 分子与细胞 组成细胞的化学元素 组成细胞的化合物</td>\n",
       "      <td>菠菜 土壤 中 吸收 氮 元素 用来 合成 淀粉 纤维素 葡萄糖 核酸 蛋白质 麦芽糖 脂肪酸</td>\n",
       "      <td>生物</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>高中 生物 稳态与环境 神经调节和体液调节的比较</td>\n",
       "      <td>下列 生物体 内 信息 传递 叙述 正确 下丘脑 分泌 促 甲状腺 激素 释放 激素 作用 ...</td>\n",
       "      <td>生物</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>高中 生物 生物技术实践 生物工程技术</td>\n",
       "      <td>自然 菌样 筛选 理想 生产 菌种 步骤 采集 菌样 富集 培养 纯种 分离 性能 测定 不...</td>\n",
       "      <td>生物</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>高中 生物 生物技术实践 生物技术在其他方面的应用 器官移植 复等位基因 胚胎移植 基因工程...</td>\n",
       "      <td>目前 精子 载体 法 逐渐 成为 具有 诱惑力 制备 转基因 动物 方法 方法 精子 外源 ...</td>\n",
       "      <td>生物</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>高中 地理 宇宙中的地球 地球运动的地理意义</td>\n",
       "      <td>某人 想 普通 飞机 一年 中 连续 两次 生日 认为 应 穿越 赤道 两级 本初子午线 国...</td>\n",
       "      <td>地理</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              labels  \\\n",
       "0                     高中 生物 分子与细胞 组成细胞的化学元素 组成细胞的化合物   \n",
       "1                           高中 生物 稳态与环境 神经调节和体液调节的比较   \n",
       "2                                高中 生物 生物技术实践 生物工程技术   \n",
       "3  高中 生物 生物技术实践 生物技术在其他方面的应用 器官移植 复等位基因 胚胎移植 基因工程...   \n",
       "4                             高中 地理 宇宙中的地球 地球运动的地理意义   \n",
       "\n",
       "                                             content subject  \n",
       "0    菠菜 土壤 中 吸收 氮 元素 用来 合成 淀粉 纤维素 葡萄糖 核酸 蛋白质 麦芽糖 脂肪酸      生物  \n",
       "1  下列 生物体 内 信息 传递 叙述 正确 下丘脑 分泌 促 甲状腺 激素 释放 激素 作用 ...      生物  \n",
       "2  自然 菌样 筛选 理想 生产 菌种 步骤 采集 菌样 富集 培养 纯种 分离 性能 测定 不...      生物  \n",
       "3  目前 精子 载体 法 逐渐 成为 具有 诱惑力 制备 转基因 动物 方法 方法 精子 外源 ...      生物  \n",
       "4  某人 想 普通 飞机 一年 中 连续 两次 生日 认为 应 穿越 赤道 两级 本初子午线 国...      地理  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "def clean_sentence(line):\n",
    "    line = re.sub(\n",
    "            \"[a-zA-Z0-9]|[\\s+\\-\\|\\!\\/\\[\\]\\{\\}_,.$%^*(+\\\"\\')]+|[:：+——()?【】《》“”！，。？、~@#￥%……&*（）]+|题目\", '',line)\n",
    "    words = jieba.cut(line, cut_all=False)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_path = \"../data/stopwords/哈工大停用词表.txt\"\n",
    "def load_stop_words(stopwords_path):\n",
    "    with open(stopwords_path, 'r') as f:\n",
    "        stop_words = [word.strip() for word in f.readlines()]\n",
    "        stop_words_dict = dict(zip(stop_words, list(range(len(stop_words)))))\n",
    "    \n",
    "    return stop_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_proc(sentence):\n",
    "    # 清除无用词\n",
    "    cw_words = list(clean_sentence(sentence))\n",
    "    # print(\"after clear sentence:\", len(list(cw_words)))\n",
    "    \n",
    "    # 去除停用词\n",
    "    sw_words = [word for word in cw_words if word not in load_stop_words(stopwords_path)]\n",
    "    # print(\"after clear stop_words:\", len(sw_words))\n",
    "    \n",
    "    return \" \".join(sw_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.543 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 5.98 s, total: 1min 35s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_df[\"labels\"] = data_df[\"labels\"].apply(sentence_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"merged\"] = data_df[[\"labels\", \"content\"]].apply(lambda x: ' '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>content</th>\n",
       "      <th>subject</th>\n",
       "      <th>merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>高中生物 分子 细胞 组成 细胞 化学元素 组成 细胞 化合物</td>\n",
       "      <td>菠菜 土壤 中 吸收 氮 元素 用来 合成 淀粉 纤维素 葡萄糖 核酸 蛋白质 麦芽糖 脂肪酸</td>\n",
       "      <td>生物</td>\n",
       "      <td>高中生物 分子 细胞 组成 细胞 化学元素 组成 细胞 化合物 菠菜 土壤 中 吸收 氮 元...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>高中生物 稳态 环境 神经 调节 体液 调节 比较</td>\n",
       "      <td>下列 生物体 内 信息 传递 叙述 正确 下丘脑 分泌 促 甲状腺 激素 释放 激素 作用 ...</td>\n",
       "      <td>生物</td>\n",
       "      <td>高中生物 稳态 环境 神经 调节 体液 调节 比较 下列 生物体 内 信息 传递 叙述 正确...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>高中生物 生物 技术 实践 生物 工程技术</td>\n",
       "      <td>自然 菌样 筛选 理想 生产 菌种 步骤 采集 菌样 富集 培养 纯种 分离 性能 测定 不...</td>\n",
       "      <td>生物</td>\n",
       "      <td>高中生物 生物 技术 实践 生物 工程技术 自然 菌样 筛选 理想 生产 菌种 步骤 采集 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>高中生物 生物 技术 实践 生物 技术 方面 应用 器官移植 复 等位基因 胚胎 移植 基因...</td>\n",
       "      <td>目前 精子 载体 法 逐渐 成为 具有 诱惑力 制备 转基因 动物 方法 方法 精子 外源 ...</td>\n",
       "      <td>生物</td>\n",
       "      <td>高中生物 生物 技术 实践 生物 技术 方面 应用 器官移植 复 等位基因 胚胎 移植 基因...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>高中地理 宇宙 中 地球 地球 运动 地理 意义</td>\n",
       "      <td>某人 想 普通 飞机 一年 中 连续 两次 生日 认为 应 穿越 赤道 两级 本初子午线 国...</td>\n",
       "      <td>地理</td>\n",
       "      <td>高中地理 宇宙 中 地球 地球 运动 地理 意义 某人 想 普通 飞机 一年 中 连续 两次...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              labels  \\\n",
       "0                    高中生物 分子 细胞 组成 细胞 化学元素 组成 细胞 化合物   \n",
       "1                          高中生物 稳态 环境 神经 调节 体液 调节 比较   \n",
       "2                              高中生物 生物 技术 实践 生物 工程技术   \n",
       "3  高中生物 生物 技术 实践 生物 技术 方面 应用 器官移植 复 等位基因 胚胎 移植 基因...   \n",
       "4                           高中地理 宇宙 中 地球 地球 运动 地理 意义   \n",
       "\n",
       "                                             content subject  \\\n",
       "0    菠菜 土壤 中 吸收 氮 元素 用来 合成 淀粉 纤维素 葡萄糖 核酸 蛋白质 麦芽糖 脂肪酸      生物   \n",
       "1  下列 生物体 内 信息 传递 叙述 正确 下丘脑 分泌 促 甲状腺 激素 释放 激素 作用 ...      生物   \n",
       "2  自然 菌样 筛选 理想 生产 菌种 步骤 采集 菌样 富集 培养 纯种 分离 性能 测定 不...      生物   \n",
       "3  目前 精子 载体 法 逐渐 成为 具有 诱惑力 制备 转基因 动物 方法 方法 精子 外源 ...      生物   \n",
       "4  某人 想 普通 飞机 一年 中 连续 两次 生日 认为 应 穿越 赤道 两级 本初子午线 国...      地理   \n",
       "\n",
       "                                              merged  \n",
       "0  高中生物 分子 细胞 组成 细胞 化学元素 组成 细胞 化合物 菠菜 土壤 中 吸收 氮 元...  \n",
       "1  高中生物 稳态 环境 神经 调节 体液 调节 比较 下列 生物体 内 信息 传递 叙述 正确...  \n",
       "2  高中生物 生物 技术 实践 生物 工程技术 自然 菌样 筛选 理想 生产 菌种 步骤 采集 ...  \n",
       "3  高中生物 生物 技术 实践 生物 技术 方面 应用 器官移植 复 等位基因 胚胎 移植 基因...  \n",
       "4  高中地理 宇宙 中 地球 地球 运动 地理 意义 某人 想 普通 飞机 一年 中 连续 两次...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv(\"../data/merged_labels_content.csv\", index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"merged\"].to_csv(\"../data/merged_labels_content_no_header.csv\", index=None, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"../data/merged_labels_content_no_header.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>高中生物 分子 细胞 组成 细胞 化学元素 组成 细胞 化合物 菠菜 土壤 中 吸收 氮 元素 用来 合成 淀粉 纤维素 葡萄糖 核酸 蛋白质 麦芽糖 脂肪酸</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>高中生物 稳态 环境 神经 调节 体液 调节 比较 下列 生物体 内 信息 传递 叙述 正确...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>高中生物 生物 技术 实践 生物 工程技术 自然 菌样 筛选 理想 生产 菌种 步骤 采集 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>高中生物 生物 技术 实践 生物 技术 方面 应用 器官移植 复 等位基因 胚胎 移植 基因...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>高中地理 宇宙 中 地球 地球 运动 地理 意义 某人 想 普通 飞机 一年 中 连续 两次...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>高中地理 宇宙 中 地球 太阳 地球 影响 太阳辐射 地球 影响 叙述 正确 太阳 放出 能...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  高中生物 分子 细胞 组成 细胞 化学元素 组成 细胞 化合物 菠菜 土壤 中 吸收 氮 元素 用来 合成 淀粉 纤维素 葡萄糖 核酸 蛋白质 麦芽糖 脂肪酸\n",
       "0  高中生物 稳态 环境 神经 调节 体液 调节 比较 下列 生物体 内 信息 传递 叙述 正确...                             \n",
       "1  高中生物 生物 技术 实践 生物 工程技术 自然 菌样 筛选 理想 生产 菌种 步骤 采集 ...                             \n",
       "2  高中生物 生物 技术 实践 生物 技术 方面 应用 器官移植 复 等位基因 胚胎 移植 基因...                             \n",
       "3  高中地理 宇宙 中 地球 地球 运动 地理 意义 某人 想 普通 飞机 一年 中 连续 两次...                             \n",
       "4  高中地理 宇宙 中 地球 太阳 地球 影响 太阳辐射 地球 影响 叙述 正确 太阳 放出 能...                             "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word2vec.LineSentence(\"../data/merged_labels_content_no_header.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['高中生物',\n",
       "  '分子',\n",
       "  '细胞',\n",
       "  '组成',\n",
       "  '细胞',\n",
       "  '化学元素',\n",
       "  '组成',\n",
       "  '细胞',\n",
       "  '化合物',\n",
       "  '菠菜',\n",
       "  '土壤',\n",
       "  '中',\n",
       "  '吸收',\n",
       "  '氮',\n",
       "  '元素',\n",
       "  '用来',\n",
       "  '合成',\n",
       "  '淀粉',\n",
       "  '纤维素',\n",
       "  '葡萄糖',\n",
       "  '核酸',\n",
       "  '蛋白质',\n",
       "  '麦芽糖',\n",
       "  '脂肪酸'],\n",
       " ['高中生物',\n",
       "  '稳态',\n",
       "  '环境',\n",
       "  '神经',\n",
       "  '调节',\n",
       "  '体液',\n",
       "  '调节',\n",
       "  '比较',\n",
       "  '下列',\n",
       "  '生物体',\n",
       "  '内',\n",
       "  '信息',\n",
       "  '传递',\n",
       "  '叙述',\n",
       "  '正确',\n",
       "  '下丘脑',\n",
       "  '分泌',\n",
       "  '促',\n",
       "  '甲状腺',\n",
       "  '激素',\n",
       "  '释放',\n",
       "  '激素',\n",
       "  '作用',\n",
       "  '甲状腺',\n",
       "  '小肠',\n",
       "  '黏膜',\n",
       "  '产生',\n",
       "  '促',\n",
       "  '胰液',\n",
       "  '素可',\n",
       "  '作用',\n",
       "  '胰岛',\n",
       "  '细胞',\n",
       "  '突触',\n",
       "  '前膜',\n",
       "  '释放',\n",
       "  '神经递质',\n",
       "  '作用',\n",
       "  '肌肉',\n",
       "  '腺体',\n",
       "  '燕麦',\n",
       "  '幼根',\n",
       "  '细胞分裂',\n",
       "  '素',\n",
       "  '含量',\n",
       "  '高',\n",
       "  '促进',\n",
       "  '乙烯',\n",
       "  '合成'],\n",
       " ['高中生物',\n",
       "  '生物',\n",
       "  '技术',\n",
       "  '实践',\n",
       "  '生物',\n",
       "  '工程技术',\n",
       "  '自然',\n",
       "  '菌样',\n",
       "  '筛选',\n",
       "  '理想',\n",
       "  '生产',\n",
       "  '菌种',\n",
       "  '步骤',\n",
       "  '采集',\n",
       "  '菌样',\n",
       "  '富集',\n",
       "  '培养',\n",
       "  '纯种',\n",
       "  '分离',\n",
       "  '性能',\n",
       "  '测定',\n",
       "  '不同',\n",
       "  '微生物',\n",
       "  '生存环境',\n",
       "  '不同',\n",
       "  '获得理想',\n",
       "  '微生物',\n",
       "  '第一步',\n",
       "  '是从',\n",
       "  '适合',\n",
       "  '环境',\n",
       "  '采集',\n",
       "  '菌样',\n",
       "  '再',\n",
       "  '一定',\n",
       "  '方法',\n",
       "  '分离',\n",
       "  '纯化',\n",
       "  '培养',\n",
       "  '嗜',\n",
       "  '盐菌',\n",
       "  '菌样',\n",
       "  '应从',\n",
       "  '环境',\n",
       "  '采集',\n",
       "  '富集',\n",
       "  '培养',\n",
       "  '指',\n",
       "  '创设',\n",
       "  '仅',\n",
       "  '适合',\n",
       "  '分离',\n",
       "  '微生物',\n",
       "  '生长',\n",
       "  '特定',\n",
       "  '环境',\n",
       "  '条件',\n",
       "  '使',\n",
       "  '数量',\n",
       "  '大大增加',\n",
       "  '分离',\n",
       "  '出所',\n",
       "  '需',\n",
       "  '微生物',\n",
       "  '培养',\n",
       "  '方法',\n",
       "  '降解',\n",
       "  '苯酚',\n",
       "  '微生物',\n",
       "  '富集',\n",
       "  '培养',\n",
       "  '应',\n",
       "  '选择',\n",
       "  '培养基',\n",
       "  '如图',\n",
       "  '分离',\n",
       "  '纯化',\n",
       "  '降解',\n",
       "  '苯酚',\n",
       "  '细菌',\n",
       "  '部分',\n",
       "  '操作步骤',\n",
       "  '请',\n",
       "  '回答',\n",
       "  '下列',\n",
       "  '问题',\n",
       "  '培养基',\n",
       "  '常用',\n",
       "  '灭菌',\n",
       "  '方法',\n",
       "  '温度',\n",
       "  '条件',\n",
       "  '下',\n",
       "  '维持',\n",
       "  '﹣',\n",
       "  '制备',\n",
       "  '培养基',\n",
       "  '时中',\n",
       "  '主要',\n",
       "  '操作',\n",
       "  '完成',\n",
       "  '步骤',\n",
       "  '操作',\n",
       "  '接种',\n",
       "  '共',\n",
       "  '需要',\n",
       "  '次',\n",
       "  '灼烧',\n",
       "  '处理',\n",
       "  '划线',\n",
       "  '平板',\n",
       "  '培养',\n",
       "  '后',\n",
       "  '第一',\n",
       "  '划线',\n",
       "  '区域',\n",
       "  '划线',\n",
       "  '上',\n",
       "  '都',\n",
       "  '不间断',\n",
       "  '长满',\n",
       "  '菌落',\n",
       "  '划线',\n",
       "  '区域',\n",
       "  '第一条',\n",
       "  '线上',\n",
       "  '无菌',\n",
       "  '落',\n",
       "  '划线',\n",
       "  '上',\n",
       "  '菌落',\n",
       "  '造成',\n",
       "  '划线',\n",
       "  '无菌',\n",
       "  '落',\n",
       "  '可能',\n",
       "  '操作失误',\n",
       "  '培养',\n",
       "  '一段时间',\n",
       "  '后',\n",
       "  '培养基',\n",
       "  '表面',\n",
       "  '形成',\n",
       "  '多个',\n",
       "  '单',\n",
       "  '菌落',\n",
       "  '不同',\n",
       "  '菌株',\n",
       "  '降解',\n",
       "  '苯酚',\n",
       "  '能力',\n",
       "  '差异',\n",
       "  '需要',\n",
       "  '做',\n",
       "  '降解',\n",
       "  '能力',\n",
       "  '性能',\n",
       "  '测定',\n",
       "  '选出',\n",
       "  '降解',\n",
       "  '能力',\n",
       "  '强',\n",
       "  '菌株',\n",
       "  '具体',\n",
       "  '思路',\n",
       "  '获得',\n",
       "  '分解',\n",
       "  '能力',\n",
       "  '强',\n",
       "  '菌种',\n",
       "  '后',\n",
       "  '临时',\n",
       "  '保存',\n",
       "  '菌种',\n",
       "  '菌种',\n",
       "  '接种',\n",
       "  '试管',\n",
       "  '培养基',\n",
       "  '上',\n",
       "  '合适',\n",
       "  '温度',\n",
       "  '下',\n",
       "  '培养',\n",
       "  '菌落',\n",
       "  '长成',\n",
       "  '后',\n",
       "  '再',\n",
       "  '放入',\n",
       "  '冰箱',\n",
       "  '保藏'],\n",
       " ['高中生物',\n",
       "  '生物',\n",
       "  '技术',\n",
       "  '实践',\n",
       "  '生物',\n",
       "  '技术',\n",
       "  '方面',\n",
       "  '应用',\n",
       "  '器官移植',\n",
       "  '复',\n",
       "  '等位基因',\n",
       "  '胚胎',\n",
       "  '移植',\n",
       "  '基因工程',\n",
       "  '概念',\n",
       "  '基因工程',\n",
       "  '原理',\n",
       "  '技术',\n",
       "  '目前',\n",
       "  '精子',\n",
       "  '载体',\n",
       "  '法',\n",
       "  '逐渐',\n",
       "  '成为',\n",
       "  '具有',\n",
       "  '诱惑力',\n",
       "  '制备',\n",
       "  '转基因',\n",
       "  '动物',\n",
       "  '方法',\n",
       "  '方法',\n",
       "  '精子',\n",
       "  '外源',\n",
       "  '基因',\n",
       "  '载体',\n",
       "  '使',\n",
       "  '精子',\n",
       "  '携带',\n",
       "  '外源',\n",
       "  '基因',\n",
       "  '进入',\n",
       "  '卵细胞',\n",
       "  '受精',\n",
       "  '如图',\n",
       "  '表示',\n",
       "  '利用',\n",
       "  '方法',\n",
       "  '制备',\n",
       "  '转基因',\n",
       "  '鼠',\n",
       "  '基本',\n",
       "  '流程',\n",
       "  '请据',\n",
       "  '图',\n",
       "  '回答',\n",
       "  '精子',\n",
       "  '载体',\n",
       "  '法',\n",
       "  '过程',\n",
       "  '中外',\n",
       "  '源',\n",
       "  '基因',\n",
       "  '能够',\n",
       "  '整合',\n",
       "  '精子',\n",
       "  '上',\n",
       "  '提高',\n",
       "  '转化率',\n",
       "  '关键',\n",
       "  '受精',\n",
       "  '时',\n",
       "  '精子',\n",
       "  '才能',\n",
       "  '进入',\n",
       "  '卵细胞',\n",
       "  '中',\n",
       "  '过程',\n",
       "  '采用',\n",
       "  '技术',\n",
       "  '卵细胞',\n",
       "  '必需',\n",
       "  '培养',\n",
       "  '时期',\n",
       "  '才能',\n",
       "  '进行',\n",
       "  '利用',\n",
       "  '转基因',\n",
       "  '动物',\n",
       "  '分泌',\n",
       "  '乳汁',\n",
       "  '生产',\n",
       "  '药物',\n",
       "  '反应器',\n",
       "  '科学家',\n",
       "  '药用',\n",
       "  '蛋白',\n",
       "  '基因',\n",
       "  '启动子',\n",
       "  '调控',\n",
       "  '组件',\n",
       "  '重组',\n",
       "  '一起',\n",
       "  '导入',\n",
       "  '哺乳动物',\n",
       "  '受精卵',\n",
       "  '中',\n",
       "  '最终',\n",
       "  '生长发育',\n",
       "  '成',\n",
       "  '转基因',\n",
       "  '动物',\n",
       "  '过程',\n",
       "  '中',\n",
       "  '形成',\n",
       "  '无毒',\n",
       "  '环境',\n",
       "  '做法',\n",
       "  '过程',\n",
       "  '通入',\n",
       "  '氧气',\n",
       "  '目的']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sentences)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22576"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Word2Vec in module gensim.models.word2vec:\n",
      "\n",
      "class Word2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      " |  Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      " |  \n",
      " |  Once you're finished training a model (=no more updates, only querying)\n",
      " |  store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in `self.wv` to reduce memory.\n",
      " |  \n",
      " |  The model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      " |  :meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      " |  \n",
      " |  The trained word vectors can also be stored/loaded from a format compatible with the\n",
      " |  original word2vec implementation via `self.wv.save_word2vec_format`\n",
      " |  and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      " |  \n",
      " |  Some important attributes are the following:\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      " |      This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      " |      directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      " |  \n",
      " |  vocabulary : :class:`~gensim.models.word2vec.Word2VecVocab`\n",
      " |      This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
      " |      Besides keeping track of all unique words, this object provides extra functionality, such as\n",
      " |      constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
      " |  \n",
      " |  trainables : :class:`~gensim.models.word2vec.Word2VecTrainables`\n",
      " |      This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
      " |      network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
      " |      a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
      " |      (which means that the size of the hidden layer is equal to the number of features `self.size`).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2Vec\n",
      " |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      " |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Deprecated. Use `self.wv.__contains__` instead.\n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__contains__`.\n",
      " |  \n",
      " |  __getitem__(self, words)\n",
      " |      Deprecated. Use `self.wv.__getitem__` instead.\n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.__getitem__`.\n",
      " |  \n",
      " |  __init__(self, sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of iterables, optional\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      " |          in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      " |      size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          Maximum distance between the current and predicted word within a sentence.\n",
      " |      min_count : int, optional\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      sg : {0, 1}, optional\n",
      " |          Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      " |      hs : {0, 1}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {0, 1}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      max_final_vocab : int, optional\n",
      " |          Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      " |          min_count is more than the calculated min_count, the specified min_count will be used.\n",
      " |          Set to `None` if not required.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      iter : int, optional\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      " |          model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      sorted_vocab : {0, 1}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      " |          See :meth:`~gensim.models.word2vec.Word2VecVocab.sort_vocab()`.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>> model = Word2Vec(sentences, min_count=1)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Human readable representation of the model's state.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      " |          and learning rate.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n",
      " |      Deprecated. Use `self.wv.accuracy` instead.\n",
      " |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.accuracy`.\n",
      " |  \n",
      " |  clear_sims(self)\n",
      " |      Remove all L2-normalized word vectors from the model, to free up memory.\n",
      " |      \n",
      " |      You can recompute them later again using the :meth:`~gensim.models.word2vec.Word2Vec.init_sims` method.\n",
      " |  \n",
      " |  delete_temporary_training_data(self, replace_word_vectors_with_normalized=False)\n",
      " |      Discard parameters that are used in training and scoring, to save memory.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      Use only if you're sure you're done training a model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace_word_vectors_with_normalized : bool, optional\n",
      " |          If True, forget the original (not normalized) word vectors and only keep\n",
      " |          the L2-normalized word vectors, to save even more memory.\n",
      " |  \n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Deprecated. Use `self.wv.init_sims` instead.\n",
      " |      See :meth:`~gensim.models.keyedvectors.Word2VecKeyedVectors.init_sims`.\n",
      " |  \n",
      " |  intersect_word2vec_format(self, fname, lockf=0.0, binary=False, encoding='utf8', unicode_errors='strict')\n",
      " |      Merge in an input-hidden weight matrix loaded from the original C word2vec-tool format,\n",
      " |      where it intersects with the current vocabulary.\n",
      " |      \n",
      " |      No words are added to the existing vocabulary, but intersecting words adopt the file's weights, and\n",
      " |      non-intersecting words are left alone.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to load the vectors from.\n",
      " |      lockf : float, optional\n",
      " |          Lock-factor value to be set for any imported word-vectors; the\n",
      " |          default value of 0.0 prevents further updating of the vector during subsequent\n",
      " |          training. Use 1.0 to allow further training updates of merged vectors.\n",
      " |      binary : bool, optional\n",
      " |          If True, `fname` is in the binary word2vec C format.\n",
      " |      encoding : str, optional\n",
      " |          Encoding of `text` for `unicode` function (python2 only).\n",
      " |      unicode_errors : str, optional\n",
      " |          Error handling behaviour, used as parameter for `unicode` function (python2 only).\n",
      " |  \n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of str\n",
      " |          List of context words.\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      " |      \n",
      " |      Structures copied are:\n",
      " |          * Vocabulary\n",
      " |          * Index to word mapping\n",
      " |          * Cumulative frequency table (used for negative sampling)\n",
      " |          * Cached corpus length\n",
      " |      \n",
      " |      Useful when testing multiple models on the same corpus in parallel.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Another model to copy the internal structures from.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the model.\n",
      " |      This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\n",
      " |      online training and getting vectors for vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the file.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False)\n",
      " |      Deprecated. Use `model.wv.save_word2vec_format` instead.\n",
      " |      See :meth:`gensim.models.KeyedVectors.save_word2vec_format`.\n",
      " |  \n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |      \n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |      \n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |      \n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |  \n",
      " |  train(self, sentences=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=())\n",
      " |      Update the model's neural weights from a sequence of sentences.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.iter`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to`train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = Word2Vec(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors\n",
      " |          (1, 30)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.word2vec.Word2Vec.save`\n",
      " |          Save model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Deprecated. Use :meth:`gensim.models.KeyedVectors.load_word2vec_format` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |      Deprecated. Use `self.wv.log_accuracy` instead.\n",
      " |      See :meth:`~gensim.models.word2vec.Word2VecKeyedVectors.log_accuracy`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  build_vocab(self, sentences=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int, optional\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs : object\n",
      " |          Key word arguments propagated to `self.vocabulary.prepare_vocab`\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Deprecated, use self.wv.doesnt_match() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vocab_size : int, optional\n",
      " |          Number of unique tokens in the vocabulary\n",
      " |      report : dict of (str, int), optional\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of (str, int)\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Deprecated, use self.wv.most_similar() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Deprecated, use self.wv.most_similar_cosmul() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Deprecated, use self.wv.n_similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_vector() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_word() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Deprecated, use self.wv.similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Deprecated, use self.wv.wmdistance() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  cum_table\n",
      " |  \n",
      " |  hashfxn\n",
      " |  \n",
      " |  iter\n",
      " |  \n",
      " |  layer1_size\n",
      " |  \n",
      " |  min_count\n",
      " |  \n",
      " |  sample\n",
      " |  \n",
      " |  syn0_lockf\n",
      " |  \n",
      " |  syn1\n",
      " |  \n",
      " |  syn1neg\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gensim.models.Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 02:49:04,197 : INFO : collecting all words and their counts\n",
      "2020-04-29 02:49:04,199 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-04-29 02:49:04,345 : INFO : PROGRESS: at sentence #10000, processed 575860 words, keeping 31793 word types\n",
      "2020-04-29 02:49:04,483 : INFO : PROGRESS: at sentence #20000, processed 1156236 words, keeping 44053 word types\n",
      "2020-04-29 02:49:04,519 : INFO : collected 46424 word types from a corpus of 1305480 raw words and 22576 sentences\n",
      "2020-04-29 02:49:04,520 : INFO : Loading a fresh vocabulary\n",
      "2020-04-29 02:49:04,546 : INFO : effective_min_count=5 retains 14244 unique words (30% of original 46424, drops 32180)\n",
      "2020-04-29 02:49:04,547 : INFO : effective_min_count=5 leaves 1249050 word corpus (95% of original 1305480, drops 56430)\n",
      "2020-04-29 02:49:04,577 : INFO : deleting the raw counts dictionary of 46424 items\n",
      "2020-04-29 02:49:04,579 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2020-04-29 02:49:04,579 : INFO : downsampling leaves estimated 1134811 word corpus (90.9% of prior 1249050)\n",
      "2020-04-29 02:49:04,602 : INFO : estimated required memory for 14244 words and 200 dimensions: 29912400 bytes\n",
      "2020-04-29 02:49:04,603 : INFO : resetting layer weights\n",
      "2020-04-29 02:49:04,727 : INFO : training model with 8 workers on 14244 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-04-29 02:49:05,747 : INFO : EPOCH 1 - PROGRESS: at 74.94% examples, 842818 words/s, in_qsize 0, out_qsize 1\n",
      "2020-04-29 02:49:05,970 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-04-29 02:49:05,978 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-04-29 02:49:05,979 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-29 02:49:05,981 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-29 02:49:05,985 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-29 02:49:05,988 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-29 02:49:05,992 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-29 02:49:06,002 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-29 02:49:06,003 : INFO : EPOCH - 1 : training on 1305480 raw words (1135069 effective words) took 1.3s, 899586 effective words/s\n",
      "2020-04-29 02:49:07,007 : INFO : EPOCH 2 - PROGRESS: at 80.18% examples, 908465 words/s, in_qsize 4, out_qsize 1\n",
      "2020-04-29 02:49:07,172 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-04-29 02:49:07,175 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-04-29 02:49:07,178 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-29 02:49:07,180 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-29 02:49:07,185 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-29 02:49:07,187 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-29 02:49:07,197 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-29 02:49:07,199 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-29 02:49:07,200 : INFO : EPOCH - 2 : training on 1305480 raw words (1135457 effective words) took 1.2s, 950698 effective words/s\n",
      "2020-04-29 02:49:08,223 : INFO : EPOCH 3 - PROGRESS: at 76.34% examples, 850767 words/s, in_qsize 8, out_qsize 1\n",
      "2020-04-29 02:49:08,443 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-04-29 02:49:08,445 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-04-29 02:49:08,446 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-29 02:49:08,448 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-29 02:49:08,451 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-29 02:49:08,459 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-29 02:49:08,461 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-29 02:49:08,465 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-29 02:49:08,466 : INFO : EPOCH - 3 : training on 1305480 raw words (1134827 effective words) took 1.3s, 900638 effective words/s\n",
      "2020-04-29 02:49:09,495 : INFO : EPOCH 4 - PROGRESS: at 81.71% examples, 904816 words/s, in_qsize 14, out_qsize 1\n",
      "2020-04-29 02:49:09,651 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-04-29 02:49:09,654 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-04-29 02:49:09,654 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-29 02:49:09,655 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-29 02:49:09,658 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-29 02:49:09,665 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-29 02:49:09,667 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-29 02:49:09,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-29 02:49:09,671 : INFO : EPOCH - 4 : training on 1305480 raw words (1134819 effective words) took 1.2s, 946139 effective words/s\n",
      "2020-04-29 02:49:10,689 : INFO : EPOCH 5 - PROGRESS: at 75.73% examples, 844386 words/s, in_qsize 14, out_qsize 1\n",
      "2020-04-29 02:49:10,852 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-04-29 02:49:10,858 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-04-29 02:49:10,861 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-29 02:49:10,864 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-29 02:49:10,869 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-29 02:49:10,873 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-29 02:49:10,875 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-29 02:49:10,881 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-29 02:49:10,882 : INFO : EPOCH - 5 : training on 1305480 raw words (1134774 effective words) took 1.2s, 939430 effective words/s\n",
      "2020-04-29 02:49:11,902 : INFO : EPOCH 6 - PROGRESS: at 76.45% examples, 852465 words/s, in_qsize 15, out_qsize 0\n",
      "2020-04-29 02:49:12,126 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-04-29 02:49:12,130 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-04-29 02:49:12,132 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-29 02:49:12,133 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-29 02:49:12,134 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-29 02:49:12,135 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-29 02:49:12,143 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-29 02:49:12,148 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-29 02:49:12,149 : INFO : EPOCH - 6 : training on 1305480 raw words (1134323 effective words) took 1.3s, 899206 effective words/s\n",
      "2020-04-29 02:49:13,182 : INFO : EPOCH 7 - PROGRESS: at 73.49% examples, 808334 words/s, in_qsize 14, out_qsize 2\n",
      "2020-04-29 02:49:13,399 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-04-29 02:49:13,403 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-04-29 02:49:13,406 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-29 02:49:13,407 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-29 02:49:13,409 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-29 02:49:13,420 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-29 02:49:13,422 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-29 02:49:13,426 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-29 02:49:13,427 : INFO : EPOCH - 7 : training on 1305480 raw words (1135082 effective words) took 1.3s, 891594 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 02:49:14,436 : INFO : EPOCH 8 - PROGRESS: at 74.94% examples, 845212 words/s, in_qsize 8, out_qsize 2\n",
      "2020-04-29 02:49:14,644 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-04-29 02:49:14,647 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-04-29 02:49:14,648 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-04-29 02:49:14,649 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-04-29 02:49:14,661 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-04-29 02:49:14,664 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-29 02:49:14,665 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-29 02:49:14,668 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-29 02:49:14,669 : INFO : EPOCH - 8 : training on 1305480 raw words (1134760 effective words) took 1.2s, 918114 effective words/s\n",
      "2020-04-29 02:49:14,670 : INFO : training on a 10443840 raw words (9079111 effective words) took 9.9s, 913260 effective words/s\n"
     ]
    }
   ],
   "source": [
    "wv_model = word2vec.Word2Vec(LineSentence(\"../data/merged_labels_content_no_header.csv\"), workers=8, min_count=5, size=200, iter=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('动物', 0.571663498878479),\n",
       " ('生物体', 0.5686122179031372),\n",
       " ('动植物', 0.5393213629722595),\n",
       " ('原核', 0.5141187906265259),\n",
       " ('植物', 0.5091243982315063),\n",
       " ('遗传物质', 0.5004448890686035),\n",
       " ('动物细胞', 0.49553942680358887),\n",
       " ('真核细胞', 0.48394379019737244),\n",
       " ('酵母菌', 0.476127952337265),\n",
       " ('细菌', 0.47345465421676636)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.wv.most_similar(['生物'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 02:51:44,312 : INFO : saving Word2Vec object under ../data/word2vec.model, separately None\n",
      "2020-04-29 02:51:44,314 : INFO : not storing attribute vectors_norm\n",
      "2020-04-29 02:51:44,316 : INFO : not storing attribute cum_table\n",
      "2020-04-29 02:51:44,590 : INFO : saved ../data/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "wv_model.save(\"../data/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14244"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 02:56:42,395 : INFO : storing 14244x200 projection weights into ../data/word2vec.bin\n"
     ]
    }
   ],
   "source": [
    "wv_model.wv.save_word2vec_format(\"../data/word2vec\" + \".bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
